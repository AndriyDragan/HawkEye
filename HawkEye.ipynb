{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detection and Classification of Military Planes: A Comparative Study of CNN, YOLO, Faster R-CNN, RetinaNet, and EfficientDet\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In the midst of the ongoing war in Ukraine, the ability to accurately detect and classify military aircraft is of critical importance for surveillance, defense, and strategic planning. This capability can significantly contribute to national security and defense efforts.\n",
        "\n",
        "Traditionally, image recognition tasks have relied on simple Convolutional Neural Networks (CNNs) for their relatively straightforward architecture and ease of implementation. However, with the availability of more complex models such as YOLO (You Only Look Once), Faster R-CNN, RetinaNet, and EfficientDet, the potential for higher accuracy and faster detection speeds has increased.\n",
        "\n",
        "This project seeks to explore and compare the performance of these advanced models against a simple CNN baseline for the task of military aircraft detection and classification. By conducting this comparative study, I aim to identify the most suitable model for practical applications in military contexts.\n",
        "\n",
        "I have decided to use a Military Aircraft Recognition dataset from the kaggle.com. This dataset include 3842 images, 20 types, and 22341 instances annotated with horizontal bounding boxes and oriented bounding boxes.\n",
        "\n",
        "In order to simplify work I have downloaded all the dataset into my git repository. Lets start by importing our project code and data from the git repository:\n"
      ],
      "metadata": {
        "id": "TO5qg9_qTCNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/AndriyDragan/HawkEye.git\n",
        "\n",
        "# Install YOLOv5\n",
        "%pip install -U ultralytics\n",
        "\n",
        "# Install EfficientDet\n",
        "%pip install -U effdet"
      ],
      "metadata": {
        "id": "S4a6qO7QokfN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysys and preparation"
      ],
      "metadata": {
        "id": "GT7vjcNDe_zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets do some import of the dependecies and look at our data:"
      ],
      "metadata": {
        "id": "WClbr0JAF04r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBLUJcItxCVY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import itertools\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.ops import box_iou\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, retinanet_resnet50_fpn\n",
        "from torchvision.models.detection.retinanet import RetinaNetHead\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from ultralytics import YOLO\n",
        "from effdet import create_model, DetBenchTrain\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "data_dir = 'HawkEye/Data'\n",
        "\n",
        "imfiles = os.listdir(os.path.join(data_dir, 'Images'))\n",
        "imfiles = [os.path.join(data_dir, 'Images', f) for f in imfiles if os.path.splitext(f)[-1] == '.jpg']\n",
        "\n",
        "def imread(filename):\n",
        "    return cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "sample = random.choice(imfiles)\n",
        "image = imread(sample)\n",
        "rows, cols, channels = image.shape\n",
        "\n",
        "plt.imshow(image)\n",
        "print('Number of samples:', len(imfiles))\n",
        "print('Image shape:      ', image.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to verify if our images have same size I will iterate all of them and clasify by shape:"
      ],
      "metadata": {
        "id": "CzdFMvb_NdkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store the count of images for each shape\n",
        "image_shapes = defaultdict(int)\n",
        "\n",
        "for imfile in tqdm(imfiles):\n",
        "    image = cv2.imread(imfile)\n",
        "    if image is not None:\n",
        "        shape = image.shape\n",
        "        image_shapes[shape] += 1\n",
        "\n",
        "# Iterate over all images and collect information about their shapes\n",
        "for shape, count in image_shapes.items():\n",
        "    print(f'Shape: {shape}, Count: {count}')"
      ],
      "metadata": {
        "id": "lgvf-4xZGPEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there is a big veriety of the image shapes yet the main claster is Shape: (800, 800, 3), Count: 3167. This will be the base of our dataset:"
      ],
      "metadata": {
        "id": "kdBvLSHjN0Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read annotations\n",
        "def read_annotations(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    annotations = []\n",
        "    for obj in root.findall('object'):\n",
        "        name = obj.find('name').text\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text)\n",
        "        ymin = int(bbox.find('ymin').text)\n",
        "        xmax = int(bbox.find('xmax').text)\n",
        "        ymax = int(bbox.find('ymax').text)\n",
        "        annotations.append((name, (xmin, ymin, xmax, ymax)))\n",
        "    return annotations\n",
        "\n",
        "\n",
        "def read_data(file_names, data_dir):\n",
        "    data = []\n",
        "    labels = []\n",
        "    dims = []\n",
        "\n",
        "    for file_name in file_names:\n",
        "        img_path = os.path.join(data_dir, 'Images', file_name + '.jpg')\n",
        "        xml_path = os.path.join(data_dir, 'Labels', 'Horizontal Bounding Boxes', file_name + '.xml')\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        if img.size != (800, 800):\n",
        "            continue\n",
        "\n",
        "        annotations = read_annotations(xml_path)\n",
        "\n",
        "        for annot in annotations:\n",
        "            width = abs(annot[1][0] - annot[1][2])\n",
        "            height = abs(annot[1][1] - annot[1][3])\n",
        "            dims.append((width, height))\n",
        "            labels.append(annot[0])\n",
        "\n",
        "        data.append((img_path, xml_path, (width, height)))\n",
        "\n",
        "    return data, labels, dims\n",
        "\n",
        "file_names = [f.split('.')[0] for f in os.listdir(os.path.join(data_dir, 'Images'))]\n",
        "data, labels, dims = read_data(file_names, data_dir)\n"
      ],
      "metadata": {
        "id": "-y5QE3TJE2uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets display some more of the random samples from our dataset, already with proper annotations:"
      ],
      "metadata": {
        "id": "Lr28zpTmIWTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display random samples\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax in axes:\n",
        "    idx = np.random.randint(0, len(data) - 1)\n",
        "    img_path, xml_path, _ = data[idx]\n",
        "    img = Image.open(img_path)\n",
        "    annotations = read_annotations(xml_path)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    for obj in annotations:\n",
        "        label, (xmin, ymin, xmax, ymax) = obj\n",
        "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red')\n",
        "        font_size = 20\n",
        "        draw.text((xmax, ymin), label, fill='red')\n",
        "\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2vWbPWDKWLW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would like to visualise some additional properties of the dataset we will work with:"
      ],
      "metadata": {
        "id": "uMnWx75sPqId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and plot the per-class histogram\n",
        "hist = Counter(labels)\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.bar(hist.keys(), hist.values())\n",
        "plt.grid(True)\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Per-Class Histogram')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xh6XxgeVe-iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the per-class distribution\n",
        "sorted_hist = dict(sorted(hist.items()))\n",
        "for class_label, count in sorted_hist.items():\n",
        "    print(f'Class {class_label}: {count} instances')"
      ],
      "metadata": {
        "id": "lr7XAh8typgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample a random subset of the dataset for brightness analysis\n",
        "sample_size = 1000\n",
        "sample_data_indices = np.random.choice(len(data), sample_size, replace=False)\n",
        "brightness = []\n",
        "\n",
        "for idx in sample_data_indices:\n",
        "    img_path = data[idx][0]\n",
        "    img = Image.open(img_path).convert('L')\n",
        "    brightness.append(np.mean(np.array(img)))\n",
        "\n",
        "sample_brightness = pd.DataFrame(brightness, columns=['Brightness'])\n",
        "\n",
        "# Plot brightness distribution for the sample\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.hist(sample_brightness['Brightness'], bins=50, alpha=0.7)\n",
        "plt.xlabel('Brightness')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Brightness Distribution (Sample)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ju4oITVxrIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Brightness distribution per class for the sample\n",
        "sample_labels = [labels[idx] for idx in sample_data_indices]\n",
        "brightness_per_class = pd.DataFrame({'ClassId': sample_labels, 'Brightness': brightness}).groupby('ClassId')['Brightness'].mean()\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.bar(brightness_per_class.index, brightness_per_class.values)\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Average Brightness')\n",
        "plt.title('Average Brightness per Class (Sample)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y83xV1plA9me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am a bit worried by severely unequal per-class distribution and would like to mitigate the risks of some classes undertraining. At first I decided that simplest pass would be to remove overepresented classes. Lets take maximum of 500 images of each class."
      ],
      "metadata": {
        "id": "4Uq8FO_RQbap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counter to count instances of each class\n",
        "def filter_data(data, class_counter, max_count):\n",
        "    new_data = []\n",
        "    new_class_counter = {class_name: 0 for class_name in class_counter}\n",
        "    for img_path, xml_path, dimensions in data:\n",
        "        annotations = read_annotations(xml_path)\n",
        "        filtered_annotations = []\n",
        "        for annot in annotations:\n",
        "            class_name = annot[0]\n",
        "            if new_class_counter[class_name] < max_count:\n",
        "                filtered_annotations.append(annot)\n",
        "                new_class_counter[class_name] += 1\n",
        "        if filtered_annotations:\n",
        "            new_data.append((img_path, xml_path, dimensions))\n",
        "    return new_data\n",
        "\n",
        "class_counter = Counter(labels)\n",
        "max_count = 500\n",
        "filtered_data = filter_data(data, class_counter, max_count)\n",
        "\n",
        "# Update labels based on filtered_data\n",
        "new_labels = []\n",
        "for img_path, xml_path, dimensions in filtered_data:\n",
        "    annotations = read_annotations(xml_path)\n",
        "    for annot in annotations:\n",
        "        new_labels.append(annot[0])\n",
        "\n",
        "# Recount instances of each class in the new dataset\n",
        "new_class_counter = Counter(new_labels)\n",
        "sorted_new_class_counter = dict(sorted(new_class_counter.items()))\n",
        "\n",
        "# Print new statistics\n",
        "for class_label, count in sorted_new_class_counter.items():\n",
        "    print(f'Class {class_label}: {count} instances')"
      ],
      "metadata": {
        "id": "6Fq4W7WcGdJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the updated per-class histogram\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.bar(sorted_new_class_counter.keys(), sorted_new_class_counter.values())\n",
        "plt.grid(True)\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Per-Class Histogram After Filtering')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lUU57sWSfV5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes I made my dataset much smaller yet I belive its optimal option to mitigate issues of class imbalance. I have no time to look for more data and using proper data augmentation is also out of scope of this project. But I will use pre-trained models and hope this data will be enough  for finetuning."
      ],
      "metadata": {
        "id": "qE1-4FA5yWPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of class names in the order of their class IDs\n",
        "class_names = ['A1', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A2', 'A20', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "random.shuffle(filtered_data)\n",
        "split_index = int(0.8 * len(filtered_data))\n",
        "train_data = filtered_data[:split_index]\n",
        "val_data = filtered_data[split_index:]\n",
        "\n",
        "# Ensure paths in train.txt and test.txt are correct\n",
        "write_data_to_file(train_data, os.path.join(data_dir, 'DataLists', 'train.txt'))\n",
        "write_data_to_file(val_data, os.path.join(data_dir, 'DataLists', 'test.txt'))\n",
        "\n",
        "# Write paths to image and XML annotation files\n",
        "def write_data_to_file(data, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        for img_path, xml_path, _ in data:\n",
        "            f.write(f\"{img_path}\\t{xml_path}\\n\")\n",
        "\n",
        "# Use new file names for image-annotation pairs to avoid overwriting\n",
        "write_data_to_file(train_data, os.path.join(data_dir, 'DataLists', 'train.txt'))\n",
        "write_data_to_file(val_data, os.path.join(data_dir, 'DataLists', 'test.txt'))"
      ],
      "metadata": {
        "id": "9Qs5s4SukEQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO"
      ],
      "metadata": {
        "id": "Nesa4v_Ot28d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Function to parse XML and convert to YOLO format\n",
        "def convert_xml_to_yolo(xml_path, img_size=(800, 800)):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    yolo_data = []\n",
        "\n",
        "    for obj in root.findall('object'):\n",
        "        name = obj.find('name').text\n",
        "        class_id = class_names.index(name)\n",
        "\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text)\n",
        "        ymin = int(bbox.find('ymin').text)\n",
        "        xmax = int(bbox.find('xmax').text)\n",
        "        ymax = int(bbox.find('ymax').text)\n",
        "\n",
        "        # Convert to YOLO format\n",
        "        x_center = (xmin + xmax) / 2 / img_size[0]\n",
        "        y_center = (ymin + ymax) / 2 / img_size[1]\n",
        "        width = (xmax - xmin) / img_size[0]\n",
        "        height = (ymax - ymin) / img_size[1]\n",
        "\n",
        "        yolo_data.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
        "\n",
        "    return yolo_data\n",
        "\n",
        "# Function to copy files based on file path lists and convert labels\n",
        "def copy_and_convert_files(file_list, img_dest, lbl_dest):\n",
        "    with open(file_list, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) != 2:\n",
        "                print(f\"Skipping line due to unexpected format: {line}\")\n",
        "                continue\n",
        "            img_path, xml_path = parts\n",
        "            shutil.copy(img_path, img_dest)\n",
        "\n",
        "            # Convert XML to YOLO and save\n",
        "            yolo_data = convert_xml_to_yolo(xml_path)\n",
        "            yolo_lbl_path = os.path.join(lbl_dest, os.path.splitext(os.path.basename(img_path))[0] + '.txt')\n",
        "            with open(yolo_lbl_path, 'w') as lbl_file:\n",
        "                lbl_file.write(\"\\n\".join(yolo_data))\n",
        "\n",
        "# Create directories for YOLO dataset\n",
        "yolo_base_dir = '/content/datasets/dataset'\n",
        "train_img_dir = os.path.join(yolo_base_dir, 'train/images')\n",
        "train_lbl_dir = os.path.join(yolo_base_dir, 'train/labels')\n",
        "val_img_dir = os.path.join(yolo_base_dir, 'val/images')\n",
        "val_lbl_dir = os.path.join(yolo_base_dir, 'val/labels')\n",
        "\n",
        "os.makedirs(train_img_dir, exist_ok=True)\n",
        "os.makedirs(train_lbl_dir, exist_ok=True)\n",
        "os.makedirs(val_img_dir, exist_ok=True)\n",
        "os.makedirs(val_lbl_dir, exist_ok=True)\n",
        "\n",
        "# Copy and convert training and validation files\n",
        "copy_and_convert_files(os.path.join(data_dir, 'DataLists', 'train.txt'), train_img_dir, train_lbl_dir)\n",
        "copy_and_convert_files(os.path.join(data_dir, 'DataLists', 'test.txt'), val_img_dir, val_lbl_dir)"
      ],
      "metadata": {
        "id": "dmtYVbuyJMM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data.yaml with correct paths\n",
        "data_yaml_content = f\"\"\"\n",
        "train: {os.path.join(yolo_base_dir, 'train')}\n",
        "val: {os.path.join(yolo_base_dir, 'val')}\n",
        "nc: {len(set(labels))}\n",
        "names: {list(set(labels))}\n",
        "\"\"\"\n",
        "\n",
        "with open('data.yaml', 'w') as f:\n",
        "    f.write(data_yaml_content)"
      ],
      "metadata": {
        "id": "y9twhq7_uGyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLO\n",
        "yolo_model = YOLO('yolov5su.pt')\n",
        "\n",
        "# Capture training history\n",
        "history = yolo_model.train(data='data.yaml', epochs=10, imgsz=800)\n",
        "\n",
        "# Validate YOLO\n",
        "yolo_results = yolo_model.val()\n",
        "print(yolo_results)\n"
      ],
      "metadata": {
        "id": "t4RXzPUHBzru",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = [x['train']['loss'] for x in history['metrics']]\n",
        "val_loss = [x['val']['loss'] for x in history['metrics']]\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, train_loss, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "DR1LwPb_9vhg",
        "outputId": "6be6aec3-2db0-44ec-c85f-4864ef9a48f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'DetMetrics' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-25f6459452ce>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DetMetrics' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster R-CNN"
      ],
      "metadata": {
        "id": "-rIPYySAYChZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, txt_file, transforms=None):\n",
        "        self.txt_file = txt_file\n",
        "        self.transforms = transforms\n",
        "        self.imgs = []\n",
        "        self.annotations = []\n",
        "        with open(txt_file) as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) != 2:\n",
        "                    print(f\"Skipping line due to unexpected format: {line}\")\n",
        "                    continue\n",
        "                img_path, xml_path = parts\n",
        "                self.imgs.append(img_path)\n",
        "                self.annotations.append(xml_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.imgs[idx]\n",
        "        xml_path = self.annotations[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        target = self.parse_xml(xml_path)\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def parse_xml(self, xml_path):\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for obj in root.findall('object'):\n",
        "            name = obj.find('name').text\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = int(bbox.find('xmin').text)\n",
        "            ymin = int(bbox.find('ymin').text)\n",
        "            xmax = int(bbox.find('xmax').text)\n",
        "            ymax = int(bbox.find('ymax').text)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(class_names.index(name) + 1)  # class label starts from 1\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        return target"
      ],
      "metadata": {
        "id": "fgSe259sYlrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = T.Compose([T.ToTensor()])\n",
        "\n",
        "train_dataset = CustomDataset(os.path.join(data_dir, 'DataLists', 'train.txt'), transforms)\n",
        "val_dataset = CustomDataset(os.path.join(data_dir, 'DataLists', 'test.txt'), transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "id": "jBSXnJUpB5YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Define the model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "num_classes = len(class_names) + 1  # number of classes + background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with history tracking\n",
        "num_epochs = 30\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, targets in train_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_epoch_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_epoch_loss}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            val_loss += losses.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_loss_history.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Validation Loss: {avg_val_loss}')\n",
        "\n",
        "# Validation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, targets in val_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        outputs = model(images)\n",
        "        print(outputs)"
      ],
      "metadata": {
        "id": "zrIN2nqsZtkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, train_loss_history, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss_history, 'r-', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zWq6ImLv-Xv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RetinaNet"
      ],
      "metadata": {
        "id": "l3V6hPS9CJYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.ops import box_iou\n",
        "from torchvision.models.detection import RetinaNet_ResNet50_FPN_Weights, retinanet_resnet50_fpn\n",
        "from torchvision.models.detection.retinanet import RetinaNetHead\n",
        "\n",
        "# Load the pre-trained RetinaNet model\n",
        "retinanet_model = retinanet_resnet50_fpn(weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT)\n",
        "num_classes = len(set(labels)) + 1  # +1 for background\n",
        "\n",
        "# Modify the classification head\n",
        "in_features = retinanet_model.head.classification_head.conv[0][0].in_channels\n",
        "num_anchors = retinanet_model.head.classification_head.num_anchors\n",
        "retinanet_model.head.classification_head = RetinaNetHead(in_features, num_anchors, num_classes)\n",
        "\n",
        "retinanet_model.to(device)"
      ],
      "metadata": {
        "id": "-V8ivVCZCAIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(retinanet_model.parameters(), lr=0.001)\n",
        "\n",
        "def compute_loss(model, images, targets):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "    matched_idxs = []\n",
        "    for target, pred in zip(targets, outputs):\n",
        "        matched_idx = box_iou(target['boxes'], pred['boxes']).max(dim=1)[1]\n",
        "        matched_idxs.append(matched_idx)\n",
        "    return model.head.compute_loss(targets, outputs, matched_idxs)\n",
        "\n",
        "# Training loop with history tracking\n",
        "num_epochs = 10\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    retinanet_model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, targets in train_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass and loss computation\n",
        "        loss_dict = compute_loss(retinanet_model, images, targets)\n",
        "\n",
        "        # Sum up all the losses\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_epoch_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {avg_epoch_loss}')\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    retinanet_model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Forward pass and loss computation\n",
        "            loss_dict = compute_loss(retinanet_model, images, targets)\n",
        "\n",
        "            # Sum up all the losses\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            val_loss += losses.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_loss_history.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Validation Loss: {avg_val_loss}')"
      ],
      "metadata": {
        "id": "LS0hCB1qcKZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, train_loss_history, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss_history, 'r-', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ECv_3heu-y6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientDet"
      ],
      "metadata": {
        "id": "cJD-z_Ydud4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from effdet import create_model, DetBenchTrain, DetBenchEval\n",
        "from effdet.evaluator import CocoEvaluator\n",
        "from effdet.data import create_loader, create_dataset, resolve_input_config\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "BknYp2XGCHRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "effdet_model = create_model('tf_efficientdet_d0', num_classes=len(class_names) + 1, pretrained=True)\n",
        "effdet_model = DetBenchTrain(effdet_model, config=effdet_model.config)\n",
        "effdet_model.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.AdamW(effdet_model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize variables to track loss history\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "# Training loop with history tracking\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    effdet_model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, targets in train_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = effdet_model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_epoch_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {avg_epoch_loss}')"
      ],
      "metadata": {
        "id": "uDGDsfafdwfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "effdet_model.eval()\n",
        "val_loss = 0\n",
        "with torch.no_grad():\n",
        "    for images, targets in val_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = effdet_model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        val_loss += losses.item()\n",
        "\n",
        "avg_val_loss = val_loss / len(val_loader)\n",
        "val_loss_history.append(avg_val_loss)\n",
        "\n",
        "print(f'Epoch {epoch + 1}, Validation Loss: {avg_val_loss}')"
      ],
      "metadata": {
        "id": "SoqBfa5fd1wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, train_loss_history, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss_history, 'r-', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aNIXJgl2_Xqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the models"
      ],
      "metadata": {
        "id": "E4gSemLVukmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate a model\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i, output in enumerate(outputs):\n",
        "                all_labels.append(targets[i]['labels'].cpu().numpy())\n",
        "                all_preds.append(output['labels'].cpu().numpy())\n",
        "\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mAP = average_precision_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    return precision, recall, f1, mAP\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, dataloader, device):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            outputs = model(images)\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    return inference_time"
      ],
      "metadata": {
        "id": "wtiLQHcgusri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLO Evaluation\n",
        "yolo_model = YOLO('path/to/your/yolo/model')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "precision, recall, f1, mAP = evaluate_model(yolo_model, val_loader, device)\n",
        "inference_time = measure_inference_time(yolo_model, val_loader, device)\n",
        "\n",
        "evaluation_results['YOLO'] = {\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'mAP': mAP,\n",
        "    'Inference Time': inference_time\n",
        "}\n",
        "\n",
        "print(evaluation_results['YOLO'])"
      ],
      "metadata": {
        "id": "CtcyqGeVkG-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faster R-CNN Evaluation\n",
        "faster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True, num_classes=len(class_names) + 1)\n",
        "faster_rcnn_model.to(device)\n",
        "\n",
        "precision, recall, f1, mAP = evaluate_model(faster_rcnn_model, val_loader, device)\n",
        "inference_time = measure_inference_time(faster_rcnn_model, val_loader, device)\n",
        "\n",
        "evaluation_results['Faster R-CNN'] = {\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'mAP': mAP,\n",
        "    'Inference Time': inference_time\n",
        "}\n",
        "\n",
        "print(evaluation_results['Faster R-CNN'])"
      ],
      "metadata": {
        "id": "pfEmq6mJkNxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RetinaNet Evaluation\n",
        "retinanet_model = retinanet_resnet50_fpn(pretrained=True)\n",
        "in_features = retinanet_model.head.classification_head.conv[0].in_channels\n",
        "num_anchors = retinanet_model.head.classification_head.num_anchors\n",
        "num_classes = len(class_names) + 1  # Include background class\n",
        "retinanet_model.head.classification_head = RetinaNetHead(in_features, num_anchors, num_classes)\n",
        "retinanet_model.to(device)\n",
        "\n",
        "precision, recall, f1, mAP = evaluate_model(retinanet_model, val_loader, device)\n",
        "inference_time = measure_inference_time(retinanet_model, val_loader, device)\n",
        "\n",
        "evaluation_results['RetinaNet'] = {\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'mAP': mAP,\n",
        "    'Inference Time': inference_time\n",
        "}\n",
        "\n",
        "print(evaluation_results['RetinaNet'])"
      ],
      "metadata": {
        "id": "WviSY-0MkSXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientDet Evaluation\n",
        "effdet_model = create_model('tf_efficientdet_d0', num_classes=len(class_names) + 1)\n",
        "effdet_model = DetBenchTrain(effdet_model, config)\n",
        "effdet_model.to(device)\n",
        "\n",
        "precision, recall, f1, mAP = evaluate_model(effdet_model, val_loader, device)\n",
        "inference_time = measure_inference_time(effdet_model, val_loader, device)\n",
        "\n",
        "evaluation_results['EfficientDet'] = {\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'mAP': mAP,\n",
        "    'Inference Time': inference_time\n",
        "}\n",
        "\n",
        "print(evaluation_results['EfficientDet'])"
      ],
      "metadata": {
        "id": "XHHG9m0_kU4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion"
      ],
      "metadata": {
        "id": "vFR1bVrKu4Ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Good data is a key! Garbge in - garbage out\n",
        "2. Basic CNN would not work for object detection and clasification, vasted time on that track"
      ],
      "metadata": {
        "id": "VdVNYb1ef1w4"
      }
    }
  ]
}