{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detection and Classification of Military Planes: A Comparative Study of YOLO, Faster R-CNN, RetinaNet, and EfficientDet\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In the midst of the ongoing war in Ukraine, the ability to accurately detect and classify military aircraft is of critical importance for surveillance, defense, and strategic planning. This capability can significantly contribute to national security and defense efforts.\n",
        "\n",
        "With the availability of advanced models such as YOLO (You Only Look Once), Faster R-CNN, RetinaNet, and EfficientDet, the potential for higher accuracy and faster detection speeds has increased.\n",
        "\n",
        "This project seeks to explore and compare the performance of these advanced models for the task of military aircraft detection and classification. By conducting this comparative study, I aim to identify the most suitable model for practical applications in military contexts.\n",
        "\n",
        "I have decided to use a Military Aircraft Recognition dataset from Kaggle. This dataset includes 3842 images, 20 types, and 22341 instances annotated with horizontal bounding boxes and oriented bounding boxes.\n",
        "\n",
        "In order to simplify the work, I have downloaded the entire dataset into my Git repository. Let's start by importing our project code and data from the Git repository:\n"
      ],
      "metadata": {
        "id": "TO5qg9_qTCNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/AndriyDragan/HawkEye.git\n",
        "\n",
        "# Install YOLOv5\n",
        "%pip install -U ultralytics\n",
        "\n",
        "# Install EfficientDet\n",
        "%pip install -U effdet"
      ],
      "metadata": {
        "id": "S4a6qO7QokfN",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "983e2eb8-cebe-45e3-93ce-152b932066ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HawkEye'...\n",
            "remote: Enumerating objects: 11653, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 11653 (delta 65), reused 5 (delta 0), pack-reused 11537\u001b[K\n",
            "Receiving objects: 100% (11653/11653), 1.15 GiB | 34.11 MiB/s, done.\n",
            "Resolving deltas: 100% (7411/7411), done.\n",
            "Updating files: 100% (11532/11532), done.\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.74-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.2.74-py3-none-any.whl (865 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.5/865.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.74 ultralytics-thop-2.0.0\n",
            "Collecting effdet\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from effdet) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet) (0.18.1+cu121)\n",
            "Collecting timm>=0.9.2 (from effdet)\n",
            "  Downloading timm-1.0.8-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet) (6.0.1)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet) (1.26.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->effdet) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->effdet) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.1->effdet) (12.6.20)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->effdet) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.1->effdet) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.1->effdet) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (2024.7.4)\n",
            "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-1.0.8-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=4d264af9b22929b63ed827cd6547a8c4ed40f4259408e16d5c491894d6706684\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, timm, effdet\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 effdet-0.4.1 omegaconf-2.3.0 timm-1.0.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "97bb6d3d95314658825104767b5a5cc5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis and Preparation"
      ],
      "metadata": {
        "id": "GT7vjcNDe_zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by importing the dependencies and examining the data:"
      ],
      "metadata": {
        "id": "WClbr0JAF04r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBLUJcItxCVY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import itertools\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.ops import box_iou\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, retinanet_resnet50_fpn\n",
        "from torchvision.models.detection.retinanet import RetinaNetHead\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from ultralytics import YOLO\n",
        "from effdet import create_model, DetBenchTrain\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "data_dir = 'HawkEye/Data'\n",
        "\n",
        "imfiles = os.listdir(os.path.join(data_dir, 'Images'))\n",
        "imfiles = [os.path.join(data_dir, 'Images', f) for f in imfiles if os.path.splitext(f)[-1] == '.jpg']\n",
        "\n",
        "def imread(filename):\n",
        "    return cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "sample = random.choice(imfiles)\n",
        "image = imread(sample)\n",
        "rows, cols, channels = image.shape\n",
        "\n",
        "plt.imshow(image)\n",
        "print('Number of samples:', len(imfiles))\n",
        "print('Image shape:      ', image.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verifying Image Sizes\n",
        "To ensure consistency, we'll check the dimensions of all images:"
      ],
      "metadata": {
        "id": "CzdFMvb_NdkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store the count of images for each shape\n",
        "image_shapes = defaultdict(int)\n",
        "\n",
        "for imfile in tqdm(imfiles):\n",
        "    image = cv2.imread(imfile)\n",
        "    if image is not None:\n",
        "        shape = image.shape\n",
        "        image_shapes[shape] += 1\n",
        "\n",
        "# Iterate over all images and collect information about their shapes\n",
        "for shape, count in image_shapes.items():\n",
        "    print(f'Shape: {shape}, Count: {count}')"
      ],
      "metadata": {
        "id": "lgvf-4xZGPEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe a variety of image shapes. The main cluster is (800, 800, 3), which will be the base of our dataset."
      ],
      "metadata": {
        "id": "kdBvLSHjN0Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Annotations"
      ],
      "metadata": {
        "id": "iK0YcbG9LPJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read annotations\n",
        "def read_annotations(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    annotations = []\n",
        "    for obj in root.findall('object'):\n",
        "        name = obj.find('name').text\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text)\n",
        "        ymin = int(bbox.find('ymin').text)\n",
        "        xmax = int(bbox.find('xmax').text)\n",
        "        ymax = int(bbox.find('ymax').text)\n",
        "        annotations.append((name, (xmin, ymin, xmax, ymax)))\n",
        "    return annotations\n",
        "\n",
        "\n",
        "def read_data(file_names, data_dir):\n",
        "    data = []\n",
        "    labels = []\n",
        "    dims = []\n",
        "\n",
        "    for file_name in file_names:\n",
        "        img_path = os.path.join(data_dir, 'Images', file_name + '.jpg')\n",
        "        xml_path = os.path.join(data_dir, 'Labels', 'Horizontal Bounding Boxes', file_name + '.xml')\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        if img.size != (800, 800):\n",
        "            continue\n",
        "\n",
        "        annotations = read_annotations(xml_path)\n",
        "\n",
        "        for annot in annotations:\n",
        "            width = abs(annot[1][0] - annot[1][2])\n",
        "            height = abs(annot[1][1] - annot[1][3])\n",
        "            dims.append((width, height))\n",
        "            labels.append(annot[0])\n",
        "\n",
        "        data.append((img_path, xml_path, (width, height)))\n",
        "\n",
        "    return data, labels, dims\n",
        "\n",
        "file_names = [f.split('.')[0] for f in os.listdir(os.path.join(data_dir, 'Images'))]\n",
        "data, labels, dims = read_data(file_names, data_dir)\n"
      ],
      "metadata": {
        "id": "-y5QE3TJE2uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying Random Samples with Annotations\n",
        "Let's display some random samples from our dataset with proper annotations:"
      ],
      "metadata": {
        "id": "Lr28zpTmIWTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display random samples\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax in axes:\n",
        "    idx = np.random.randint(0, len(data) - 1)\n",
        "    img_path, xml_path, _ = data[idx]\n",
        "    img = Image.open(img_path)\n",
        "    annotations = read_annotations(xml_path)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    for obj in annotations:\n",
        "        label, (xmin, ymin, xmax, ymax) = obj\n",
        "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red')\n",
        "        font_size = 20\n",
        "        draw.text((xmax, ymin), label, fill='red')\n",
        "\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2vWbPWDKWLW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will visualize additional properties of the dataset, such as per-class histogram and brightness distribution:"
      ],
      "metadata": {
        "id": "uMnWx75sPqId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and plot the per-class histogram\n",
        "hist = Counter(labels)\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.bar(hist.keys(), hist.values())\n",
        "plt.grid(True)\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Per-Class Histogram')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xh6XxgeVe-iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the per-class distribution\n",
        "sorted_hist = dict(sorted(hist.items()))\n",
        "for class_label, count in sorted_hist.items():\n",
        "    print(f'Class {class_label}: {count} instances')"
      ],
      "metadata": {
        "id": "lr7XAh8typgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample a random subset of the dataset for brightness analysis\n",
        "sample_size = 1000\n",
        "sample_data_indices = np.random.choice(len(data), sample_size, replace=False)\n",
        "brightness = []\n",
        "\n",
        "for idx in sample_data_indices:\n",
        "    img_path = data[idx][0]\n",
        "    img = Image.open(img_path).convert('L')\n",
        "    brightness.append(np.mean(np.array(img)))\n",
        "\n",
        "sample_brightness = pd.DataFrame(brightness, columns=['Brightness'])\n",
        "\n",
        "# Plot brightness distribution for the sample\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.hist(sample_brightness['Brightness'], bins=50, alpha=0.7)\n",
        "plt.xlabel('Brightness')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Brightness Distribution (Sample)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ju4oITVxrIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Brightness distribution per class for the sample\n",
        "sample_labels = [labels[idx] for idx in sample_data_indices]\n",
        "brightness_per_class = pd.DataFrame({'ClassId': sample_labels, 'Brightness': brightness}).groupby('ClassId')['Brightness'].mean()\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.bar(brightness_per_class.index, brightness_per_class.values)\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Average Brightness')\n",
        "plt.title('Average Brightness per Class (Sample)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y83xV1plA9me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am a bit worried by severely unequal per-class distribution and would like to mitigate the risks of some classes undertraining. At first I decided that simplest pass would be to remove overepresented classes. Lets take maximum of 500 images of each class."
      ],
      "metadata": {
        "id": "4Uq8FO_RQbap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counter to count instances of each class\n",
        "def filter_data(data, class_counter, max_count):\n",
        "    new_data = []\n",
        "    new_class_counter = {class_name: 0 for class_name in class_counter}\n",
        "    for img_path, xml_path, dimensions in data:\n",
        "        annotations = read_annotations(xml_path)\n",
        "        filtered_annotations = []\n",
        "        for annot in annotations:\n",
        "            class_name = annot[0]\n",
        "            if new_class_counter[class_name] < max_count:\n",
        "                filtered_annotations.append(annot)\n",
        "                new_class_counter[class_name] += 1\n",
        "        if filtered_annotations:\n",
        "            new_data.append((img_path, xml_path, dimensions))\n",
        "    return new_data\n",
        "\n",
        "class_counter = Counter(labels)\n",
        "max_count = 500\n",
        "filtered_data = filter_data(data, class_counter, max_count)\n",
        "\n",
        "# Update labels based on filtered_data\n",
        "new_labels = []\n",
        "for img_path, xml_path, dimensions in filtered_data:\n",
        "    annotations = read_annotations(xml_path)\n",
        "    for annot in annotations:\n",
        "        new_labels.append(annot[0])\n",
        "\n",
        "# Recount instances of each class in the new dataset\n",
        "new_class_counter = Counter(new_labels)\n",
        "sorted_new_class_counter = dict(sorted(new_class_counter.items()))\n",
        "\n",
        "# Print new statistics\n",
        "for class_label, count in sorted_new_class_counter.items():\n",
        "    print(f'Class {class_label}: {count} instances')"
      ],
      "metadata": {
        "id": "6Fq4W7WcGdJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the updated per-class histogram\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.bar(sorted_new_class_counter.keys(), sorted_new_class_counter.values())\n",
        "plt.grid(True)\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Per-Class Histogram After Filtering')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lUU57sWSfV5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes I made my dataset much smaller yet I belive its optimal option to mitigate issues of class imbalance. I have no time to look for more data and using proper data augmentation is also out of scope of this project. But I will use pre-trained models and hope this data will be enough  for finetuning."
      ],
      "metadata": {
        "id": "qE1-4FA5yWPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of class names in the order of their class IDs\n",
        "class_names = ['A1', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A2', 'A20', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "random.shuffle(filtered_data)\n",
        "split_index = int(0.8 * len(filtered_data))\n",
        "train_data = filtered_data[:split_index]\n",
        "val_data = filtered_data[split_index:]\n",
        "\n",
        "# Ensure paths in train.txt and test.txt are correct\n",
        "write_data_to_file(train_data, os.path.join(data_dir, 'DataLists', 'train.txt'))\n",
        "write_data_to_file(val_data, os.path.join(data_dir, 'DataLists', 'test.txt'))\n",
        "\n",
        "# Write paths to image and XML annotation files\n",
        "def write_data_to_file(data, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        for img_path, xml_path, _ in data:\n",
        "            f.write(f\"{img_path}\\t{xml_path}\\n\")\n",
        "\n",
        "# Use new file names for image-annotation pairs to avoid overwriting\n",
        "write_data_to_file(train_data, os.path.join(data_dir, 'DataLists', 'train.txt'))\n",
        "write_data_to_file(val_data, os.path.join(data_dir, 'DataLists', 'test.txt'))"
      ],
      "metadata": {
        "id": "9Qs5s4SukEQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO"
      ],
      "metadata": {
        "id": "Nesa4v_Ot28d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Function to parse XML and convert to YOLO format\n",
        "def convert_xml_to_yolo(xml_path, img_size=(800, 800)):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    yolo_data = []\n",
        "\n",
        "    for obj in root.findall('object'):\n",
        "        name = obj.find('name').text\n",
        "        class_id = class_names.index(name)\n",
        "\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text)\n",
        "        ymin = int(bbox.find('ymin').text)\n",
        "        xmax = int(bbox.find('xmax').text)\n",
        "        ymax = int(bbox.find('ymax').text)\n",
        "\n",
        "        # Convert to YOLO format\n",
        "        x_center = (xmin + xmax) / 2 / img_size[0]\n",
        "        y_center = (ymin + ymax) / 2 / img_size[1]\n",
        "        width = (xmax - xmin) / img_size[0]\n",
        "        height = (ymax - ymin) / img_size[1]\n",
        "\n",
        "        yolo_data.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
        "\n",
        "    return yolo_data\n",
        "\n",
        "# Function to copy files based on file path lists and convert labels\n",
        "def copy_and_convert_files(file_list, img_dest, lbl_dest):\n",
        "    with open(file_list, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) != 2:\n",
        "                print(f\"Skipping line due to unexpected format: {line}\")\n",
        "                continue\n",
        "            img_path, xml_path = parts\n",
        "            shutil.copy(img_path, img_dest)\n",
        "\n",
        "            # Convert XML to YOLO and save\n",
        "            yolo_data = convert_xml_to_yolo(xml_path)\n",
        "            yolo_lbl_path = os.path.join(lbl_dest, os.path.splitext(os.path.basename(img_path))[0] + '.txt')\n",
        "            with open(yolo_lbl_path, 'w') as lbl_file:\n",
        "                lbl_file.write(\"\\n\".join(yolo_data))\n",
        "\n",
        "# Create directories for YOLO dataset\n",
        "yolo_base_dir = '/content/datasets/dataset'\n",
        "train_img_dir = os.path.join(yolo_base_dir, 'train/images')\n",
        "train_lbl_dir = os.path.join(yolo_base_dir, 'train/labels')\n",
        "val_img_dir = os.path.join(yolo_base_dir, 'val/images')\n",
        "val_lbl_dir = os.path.join(yolo_base_dir, 'val/labels')\n",
        "\n",
        "os.makedirs(train_img_dir, exist_ok=True)\n",
        "os.makedirs(train_lbl_dir, exist_ok=True)\n",
        "os.makedirs(val_img_dir, exist_ok=True)\n",
        "os.makedirs(val_lbl_dir, exist_ok=True)\n",
        "\n",
        "# Copy and convert training and validation files\n",
        "copy_and_convert_files(os.path.join(data_dir, 'DataLists', 'train.txt'), train_img_dir, train_lbl_dir)\n",
        "copy_and_convert_files(os.path.join(data_dir, 'DataLists', 'test.txt'), val_img_dir, val_lbl_dir)"
      ],
      "metadata": {
        "id": "dmtYVbuyJMM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data.yaml with correct paths\n",
        "data_yaml_content = f\"\"\"\n",
        "train: {os.path.join(yolo_base_dir, 'train')}\n",
        "val: {os.path.join(yolo_base_dir, 'val')}\n",
        "nc: {len(set(labels))}\n",
        "names: {list(set(labels))}\n",
        "\"\"\"\n",
        "\n",
        "with open('data.yaml', 'w') as f:\n",
        "    f.write(data_yaml_content)"
      ],
      "metadata": {
        "id": "y9twhq7_uGyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLO\n",
        "yolo_model = YOLO('yolov5su.pt')\n",
        "\n",
        "# Capture training history\n",
        "history = yolo_model.train(data='data.yaml', epochs=10, imgsz=800)\n",
        "\n",
        "# Validate YOLO\n",
        "yolo_results = yolo_model.val()\n",
        "print(yolo_results)\n"
      ],
      "metadata": {
        "id": "t4RXzPUHBzru",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = [x['train']['loss'] for x in history['metrics']]\n",
        "val_loss = [x['val']['loss'] for x in history['metrics']]\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, train_loss, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "DR1LwPb_9vhg",
        "outputId": "6be6aec3-2db0-44ec-c85f-4864ef9a48f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'DetMetrics' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-25f6459452ce>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DetMetrics' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster R-CNN"
      ],
      "metadata": {
        "id": "-rIPYySAYChZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, txt_file, transforms=None):\n",
        "        self.txt_file = txt_file\n",
        "        self.transforms = transforms\n",
        "        self.imgs = []\n",
        "        self.annotations = []\n",
        "        with open(txt_file) as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) != 2:\n",
        "                    print(f\"Skipping line due to unexpected format: {line}\")\n",
        "                    continue\n",
        "                img_path, xml_path = parts\n",
        "                self.imgs.append(img_path)\n",
        "                self.annotations.append(xml_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.imgs[idx]\n",
        "        xml_path = self.annotations[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        target = self.parse_xml(xml_path)\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def parse_xml(self, xml_path):\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for obj in root.findall('object'):\n",
        "            name = obj.find('name').text\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = int(bbox.find('xmin').text)\n",
        "            ymin = int(bbox.find('ymin').text)\n",
        "            xmax = int(bbox.find('xmax').text)\n",
        "            ymax = int(bbox.find('ymax').text)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(class_names.index(name) + 1)  # class label starts from 1\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        return target"
      ],
      "metadata": {
        "id": "fgSe259sYlrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = T.Compose([T.ToTensor()])\n",
        "\n",
        "train_dataset = CustomDataset(os.path.join(data_dir, 'DataLists', 'train.txt'), transforms)\n",
        "val_dataset = CustomDataset(os.path.join(data_dir, 'DataLists', 'test.txt'), transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "id": "jBSXnJUpB5YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Define the model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "num_classes = len(class_names) + 1  # number of classes + background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with history tracking\n",
        "num_epochs = 30\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, targets in train_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_epoch_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_epoch_loss}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            val_loss += losses.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_loss_history.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Validation Loss: {avg_val_loss}')\n",
        "\n",
        "# Validation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, targets in val_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        outputs = model(images)\n",
        "        print(outputs)"
      ],
      "metadata": {
        "id": "zrIN2nqsZtkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, train_loss_history, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss_history, 'r-', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zWq6ImLv-Xv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RetinaNet"
      ],
      "metadata": {
        "id": "l3V6hPS9CJYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.ops import box_iou\n",
        "from torchvision.models.detection import RetinaNet_ResNet50_FPN_Weights, retinanet_resnet50_fpn\n",
        "from torchvision.models.detection.retinanet import RetinaNetHead\n",
        "\n",
        "# Load the pre-trained RetinaNet model\n",
        "retinanet_model = retinanet_resnet50_fpn(weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT)\n",
        "num_classes = len(set(labels)) + 1  # +1 for background\n",
        "\n",
        "# Modify the classification head\n",
        "in_features = retinanet_model.head.classification_head.conv[0][0].in_channels\n",
        "num_anchors = retinanet_model.head.classification_head.num_anchors\n",
        "retinanet_model.head.classification_head = RetinaNetHead(in_features, num_anchors, num_classes)\n",
        "\n",
        "retinanet_model.to(device)"
      ],
      "metadata": {
        "id": "-V8ivVCZCAIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(retinanet_model.parameters(), lr=0.001)\n",
        "\n",
        "def compute_loss(model, images, targets):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "    matched_idxs = []\n",
        "    for target, pred in zip(targets, outputs):\n",
        "        matched_idx = box_iou(target['boxes'], pred['boxes']).max(dim=1)[1]\n",
        "        matched_idxs.append(matched_idx)\n",
        "    return model.head.compute_loss(targets, outputs, matched_idxs)\n",
        "\n",
        "# Training loop with history tracking\n",
        "num_epochs = 10\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    retinanet_model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, targets in train_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass and loss computation\n",
        "        loss_dict = compute_loss(retinanet_model, images, targets)\n",
        "\n",
        "        # Sum up all the losses\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_epoch_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {avg_epoch_loss}')\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    retinanet_model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Forward pass and loss computation\n",
        "            loss_dict = compute_loss(retinanet_model, images, targets)\n",
        "\n",
        "            # Sum up all the losses\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            val_loss += losses.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_loss_history.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Validation Loss: {avg_val_loss}')"
      ],
      "metadata": {
        "id": "LS0hCB1qcKZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, train_loss_history, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss_history, 'r-', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ECv_3heu-y6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientDet"
      ],
      "metadata": {
        "id": "cJD-z_Ydud4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from effdet import create_model, DetBenchTrain, DetBenchEval\n",
        "from effdet.evaluator import CocoEvaluator\n",
        "from effdet.data import create_loader, create_dataset, resolve_input_config\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "BknYp2XGCHRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "effdet_model = create_model('tf_efficientdet_d0', num_classes=len(class_names) + 1, pretrained=True)\n",
        "effdet_model = DetBenchTrain(effdet_model, config=effdet_model.config)\n",
        "effdet_model.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.AdamW(effdet_model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize variables to track loss history\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "# Training loop with history tracking\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    effdet_model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, targets in train_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = effdet_model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_epoch_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {avg_epoch_loss}')"
      ],
      "metadata": {
        "id": "uDGDsfafdwfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "effdet_model.eval()\n",
        "val_loss = 0\n",
        "with torch.no_grad():\n",
        "    for images, targets in val_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = effdet_model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        val_loss += losses.item()\n",
        "\n",
        "avg_val_loss = val_loss / len(val_loader)\n",
        "val_loss_history.append(avg_val_loss)\n",
        "\n",
        "print(f'Epoch {epoch + 1}, Validation Loss: {avg_val_loss}')"
      ],
      "metadata": {
        "id": "SoqBfa5fd1wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, train_loss_history, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss_history, 'r-', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aNIXJgl2_Xqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the models"
      ],
      "metadata": {
        "id": "E4gSemLVukmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate a model\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i, output in enumerate(outputs):\n",
        "                all_labels.append(targets[i]['labels'].cpu().numpy())\n",
        "                all_preds.append(output['labels'].cpu().numpy())\n",
        "\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mAP = average_precision_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    return precision, recall, f1, mAP\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, dataloader, device):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            outputs = model(images)\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    return inference_time"
      ],
      "metadata": {
        "id": "wtiLQHcgusri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLO Evaluation\n",
        "yolo_model = YOLO('path/to/your/yolo/model')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "precision, recall, f1, mAP = evaluate_model(yolo_model, val_loader, device)\n",
        "inference_time = measure_inference_time(yolo_model, val_loader, device)\n",
        "\n",
        "evaluation_results['YOLO'] = {\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'mAP': mAP,\n",
        "    'Inference Time': inference_time\n",
        "}\n",
        "\n",
        "print(evaluation_results['YOLO'])"
      ],
      "metadata": {
        "id": "CtcyqGeVkG-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faster R-CNN Evaluation\n",
        "faster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True, num_classes=len(class_names) + 1)\n",
        "faster_rcnn_model.to(device)\n",
        "\n",
        "precision, recall, f1, mAP = evaluate_model(faster_rcnn_model, val_loader, device)\n",
        "inference_time = measure_inference_time(faster_rcnn_model, val_loader, device)\n",
        "\n",
        "evaluation_results['Faster R-CNN'] = {\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'mAP': mAP,\n",
        "    'Inference Time': inference_time\n",
        "}\n",
        "\n",
        "print(evaluation_results['Faster R-CNN'])"
      ],
      "metadata": {
        "id": "pfEmq6mJkNxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RetinaNet Evaluation\n",
        "retinanet_model = retinanet_resnet50_fpn(pretrained=True)\n",
        "in_features = retinanet_model.head.classification_head.conv[0].in_channels\n",
        "num_anchors = retinanet_model.head.classification_head.num_anchors\n",
        "num_classes = len(class_names) + 1  # Include background class\n",
        "retinanet_model.head.classification_head = RetinaNetHead(in_features, num_anchors, num_classes)\n",
        "retinanet_model.to(device)\n",
        "\n",
        "precision, recall, f1, mAP = evaluate_model(retinanet_model, val_loader, device)\n",
        "inference_time = measure_inference_time(retinanet_model, val_loader, device)\n",
        "\n",
        "evaluation_results['RetinaNet'] = {\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'mAP': mAP,\n",
        "    'Inference Time': inference_time\n",
        "}\n",
        "\n",
        "print(evaluation_results['RetinaNet'])"
      ],
      "metadata": {
        "id": "WviSY-0MkSXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientDet Evaluation\n",
        "effdet_model = create_model('tf_efficientdet_d0', num_classes=len(class_names) + 1)\n",
        "effdet_model = DetBenchTrain(effdet_model, config)\n",
        "effdet_model.to(device)\n",
        "\n",
        "precision, recall, f1, mAP = evaluate_model(effdet_model, val_loader, device)\n",
        "inference_time = measure_inference_time(effdet_model, val_loader, device)\n",
        "\n",
        "evaluation_results['EfficientDet'] = {\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'mAP': mAP,\n",
        "    'Inference Time': inference_time\n",
        "}\n",
        "\n",
        "print(evaluation_results['EfficientDet'])"
      ],
      "metadata": {
        "id": "XHHG9m0_kU4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion"
      ],
      "metadata": {
        "id": "vFR1bVrKu4Ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Good data is a key! Garbge in - garbage out\n",
        "2. Basic CNN would not work for object detection and clasification, vasted time on that track"
      ],
      "metadata": {
        "id": "VdVNYb1ef1w4"
      }
    }
  ]
}